# reproducibility
seed: 42

# model name
model_name: multitask-bart-large-binary-rhyming-recognition-no-classifier-refactored-last-token-fixed-no-input 
# used to name the directory in which model's checkpoints will be stored (experiments/model_name/...)
lr: 1e-5
description: "Bart-large model with multitask heads for rhyming recognition refactored. Rhyming vectors are taken from last tokens not from sentence_end_id. Generates only the output, does not repeat the input. Training on items with rhymes therein. Classification is done with dot product between decoder hidden vectors of rhyming and non-rhyming tokns."
maked_decoder_input_ids_training: False
# pl_trainer
pl_trainer:
  _target_: pytorch_lightning.Trainer
  gpus: [0,]
  accumulate_grad_batches: 16
  limit_val_batches: 50
  gradient_clip_val: 10.0
  val_check_interval: 10_000  # you can specify an int "n" here => validation every "n" steps
  max_steps: 1_000_000
  num_sanity_val_steps: 2
  # strategy: ddp
  # num_nodes: 1
  # uncomment the lines below for training with mixed precision
  precision: 16

# early stopping callback
# "early_stopping_callback: null" will disable early stopping
early_stopping_callback:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: main_val_loss
  mode: min
  patience: 5

model_checkpoint_callback:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  monitor: val_loss
  mode: min
  verbose: True
  save_top_k: 2
  dirpath: /lyrics_generation/NEW_CKPT_CHECK/
